---
date: 2026-02-11
summary: "Replacing single-snapshot cache with OrderedDict LRU, removing aggregation, and adding in-memory EDIT/annotate snapshot patching with oracle verification"
audience: intermediate
---

# Phase 1.4: LRU Compile Cache and Snapshot Patching

Phase 1.4 is a purely internal refactor — no schema migrations, no new public API surface (beyond a `verify_cache` debug flag). But it fundamentally changes how the compile cache works, and understanding *why* these changes are synergistic is the key to understanding this phase.

## Why Phase 1.4 Exists

Phase 1.1 introduced the compile cache: a single `CompileSnapshot` stored on the Tract instance, invalidated whenever something changed. That was a solid first pass, but two problems became apparent as we built on top of it.

**Problem 1: Aggregation destroyed event boundaries.** The Phase 1.1 compiler merged consecutive same-role messages into one. If you had three consecutive user messages from three separate commits, aggregation collapsed them into a single message. This meant there was no reliable way to map a commit hash back to a specific message in the compiled output — the 1:1 correspondence was gone. Without that correspondence, EDIT patching (finding the message a commit produced and replacing it in-place) was impossible. We'd have to fully recompile on every EDIT, which defeated the purpose of caching.

**Problem 2: Single-snapshot cache couldn't survive HEAD switches.** The Phase 1.1 cache stored exactly one snapshot, keyed to the current HEAD. Any time HEAD changed — a checkout, a reset, even compiling at a different commit via time-travel — the cache was invalidated. In a world with branching (Phase 2), you'd constantly bounce between branches and pay full recompile cost every time. Even in linear history, operations like `annotate()` and `batch()` cleared the cache entirely.

These two problems are synergistic: removing aggregation makes commit-to-message mapping trivial (enabling EDIT patching), and replacing the single snapshot with an LRU cache means HEAD switches can hit cached entries instead of recompiling. Solving them together is cleaner than solving either alone.

## Design Decisions

### Remove Aggregation Entirely

| Decision | Alternative | Why This Way |
|----------|-------------|--------------|
| No aggregation; consecutive same-role messages stay separate | Keep aggregation, track original boundaries | Aggregation adds complexity for zero user benefit. Commits are discrete events — a user message, a tool result, an instruction. Merging them destroys provenance. Without aggregation, `commit_hashes[i]` trivially corresponds to `messages[i]`. |

This is a philosophical decision as much as a technical one. In git, each commit is a discrete unit of work. In Trace, each commit is a discrete conversational event. Aggregation treated messages as a stream to be optimized; we treat them as an event log to be preserved. The `_aggregate_messages()` method was deleted entirely from `DefaultContextCompiler`, not just disabled.

The immediate consequence: `CompileSnapshot` no longer needs separate `raw_messages` and `aggregated_messages` fields. It simplifies to `messages` (what gets returned) and `commit_hashes` (which commit produced each message). These two tuples are parallel — same length, same order, `commit_hashes[i]` is the commit that produced `messages[i]`.

### OrderedDict LRU Cache

| Decision | Alternative | Why This Way |
|----------|-------------|--------------|
| `collections.OrderedDict` with manual LRU logic | `functools.lru_cache` | `lru_cache` decorates functions and hashes arguments — awkward for mutable state on an instance. OrderedDict gives us `move_to_end()` and `popitem(last=False)` for O(1) LRU operations with full control over eviction. |
| | Plain `dict` + timestamp tracking | More code, worse performance. OrderedDict's insertion-order tracking is exactly what LRU needs. |
| | Third-party LRU library | No external dependencies needed for something this simple. OrderedDict is stdlib and C-optimized in CPython. |

The cache is keyed by `head_hash` (the HEAD commit at compile time) and stores `CompileSnapshot` values. Default maxsize is 8, configurable via `TractConfig.compile_cache_maxsize`. Eight slots covers the typical pattern of a main branch plus a few feature branches plus a few recent checkouts. The cache lives on the `Tract` instance, so it's per-tract and garbage-collected when the Tract is closed.

### EDIT: In-Memory Patching, No Cache Clear

| Decision | Alternative | Why This Way |
|----------|-------------|--------------|
| Find target message by `commit_hashes.index()`, replace in-place, recount tokens | Full cache invalidation (Phase 1.1 behavior) | EDIT replaces one message. We know exactly which one (the commit it targets via `response_to`). Rebuilding the entire snapshot from DB is wasteful when we can patch a single position. |
| Don't clear other cache entries on EDIT | Clear entire cache | An EDIT creates a new HEAD. Other cached snapshots (at different HEADs) are unaffected — the EDIT commit isn't in their chains. Keeping them means a future checkout back to that HEAD is a cache hit. |

The patching algorithm handles `generation_config` inheritance correctly: if the EDIT commit specifies a config, it replaces the original's; if it doesn't, the original commit's config is preserved. This is the "edit-inherits-original" rule from Phase 1.3, now applied at the cache level.

### Annotate: SKIP-Only Patching with Asymmetric Fallback

| Decision | Alternative | Why This Way |
|----------|-------------|--------------|
| SKIP annotation: remove message from snapshot | Full invalidation | Removing an element from a tuple is cheap. We know the position from `commit_hashes`. |
| Un-skip (NORMAL/PINNED on previously-skipped commit): fall back to full recompile | Store all messages including skipped ones | Un-skip is rare. Storing skipped messages adds complexity for a case that almost never happens. Future phases could add a "skip mask" if this becomes a bottleneck. |
| Clear entire cache on annotate, then re-add patched current HEAD | Selective invalidation of affected entries | An annotation can affect any snapshot that contains the annotated commit. Figuring out which cached entries contain a given commit requires scanning all of them. Clearing everything and re-adding just the current HEAD is simple and correct. |

The asymmetry here is deliberate: SKIP is common (hiding a message from context), un-skip is rare (restoring a hidden message). We optimize for the common case and accept a full recompile for the rare one.

### batch() Clears Everything

`batch()` performs multiple commits atomically. Rather than tracking which cache entries might be affected by each commit in the batch, we clear the entire cache on batch entry. This is simple, correct, and batch operations are infrequent enough that the cost is negligible.

### verify_cache: Oracle Testing Flag

| Decision | Alternative | Why This Way |
|----------|-------------|--------------|
| `verify_cache=True` on `Tract.open()` / `Tract.from_components()` | Separate test harness | The flag lives on the Tract instance, so every `compile()` call automatically double-checks. No special test wiring needed — just pass the flag and run your normal test suite. |
| Default `False` | Default `True` | Oracle verification doubles compile cost (full recompile on every cache hit). Fine for tests, unacceptable for production. |

When `verify_cache=True`, every cache hit triggers a parallel full recompile from the database. The cached result and the fresh result are compared for message equality and token count equality. If they diverge, an `AssertionError` fires immediately, pinpointing exactly which compile call produced a stale cache result. This caught real bugs during development.

## Implementation Walkthrough

### CompileSnapshot Simplification

The snapshot dataclass in `src/tract/protocols.py` went from five content fields to a clean parallel structure:

```python
@dataclass(frozen=True)
class CompileSnapshot:
    head_hash: str
    messages: tuple[Message, ...]
    commit_count: int
    token_count: int
    token_source: str
    generation_configs: tuple[dict, ...] = ()
    commit_hashes: tuple[str, ...] = ()
```

`messages` and `commit_hashes` are the core parallel tuples. `commit_hashes[i]` is the hash of the commit that produced `messages[i]`. Both are tuples (immutable) because `CompileSnapshot` is `frozen=True` — all modifications go through `dataclasses.replace()`, which creates a new instance with specified fields overridden.

`CompiledContext` (the user-facing output) gained a matching `commit_hashes: list[str]` field. When converting from snapshot to compiled context, tuples become lists (mutable, for user convenience) and `generation_configs` dicts are shallow-copied (cache safety).

### LRU Cache Helpers

Three private methods on `Tract` in `src/tract/tract.py` manage the cache:

**`_cache_get(head_hash)`** looks up a snapshot by HEAD hash. On hit, it calls `move_to_end(head_hash)` to mark the entry as recently used (moving it to the back of the OrderedDict's insertion order). Returns `None` on miss.

**`_cache_put(head_hash, snapshot)`** stores a snapshot. If the key already exists, it's moved to the end (update in place). If the cache is at capacity, `popitem(last=False)` evicts the least-recently-used entry (the one at the front of the OrderedDict). This is the classic LRU pattern: most-recent at the back, least-recent at the front, evict from the front.

**`_cache_clear()`** empties the entire cache. Called by `batch()` on entry and by `annotate()` before re-adding the patched current HEAD.

The `__init__` sets up the cache from `TractConfig`:

```python
self._snapshot_cache: OrderedDict[str, CompileSnapshot] = OrderedDict()
self._snapshot_cache_maxsize: int = config.compile_cache_maxsize
```

### APPEND: Incremental Extension

When `commit()` creates an APPEND commit, it checks whether the parent HEAD has a cached snapshot. If so, it extends the snapshot rather than recompiling:

1. Capture `prev_head` before the commit writes to storage
2. After storage write, look up `_cache_get(prev_head)`
3. If hit: build a single message from the new commit via `compiler.build_message_for_commit()`
4. Append the new message and commit hash to the snapshot's tuples
5. Recount tokens across all messages (necessary because message framing overhead depends on position)
6. Store the extended snapshot under the new HEAD hash via `_cache_put()`

The parent snapshot stays in the cache too — it isn't evicted unless the LRU is full. This means a future checkout back to the parent HEAD would be a cache hit, unlike Phase 1.1 where the old snapshot was overwritten.

Token recounting is O(n) where n is the number of messages, but n is bounded by the context window size (typically a few hundred messages at most). The alternative — incremental token math — would require tracking per-message token counts and message-framing overhead separately, which adds complexity for marginal gain.

### EDIT: In-Memory Patching

When `commit()` creates an EDIT commit, the patching flow in `_patch_snapshot_for_edit()` is:

1. **Validate preconditions:** The snapshot must have `commit_hashes`, and the EDIT commit must have a `response_to` target. If either is missing, return `None` to signal a full recompile fallback.

2. **Find the target position:** Linear scan through `commit_hashes` to find the index of the target commit. The scan is O(n) but n is small (bounded by context window). If not found, return `None`.

3. **Build the replacement message:** Call `compiler.build_message_for_commit(edit_row)` to construct the new message from the EDIT commit's content. This typically hits the SQLAlchemy session cache since the commit was just written in the same session.

4. **Handle generation_config inheritance:** If the EDIT commit specifies a `generation_config`, use it (with a shallow copy for cache safety). If it doesn't, keep the original commit's config from the snapshot. This preserves the Phase 1.3 "edit-inherits-original" rule.

5. **Replace and recount:** Swap the message at the target index, recount tokens across all messages, and return a new snapshot with the updated HEAD hash.

The critical detail: EDIT patching does *not* clear other cache entries. If you have snapshots cached at heads A, B, and C, and you EDIT a commit creating head D (branching off C), the snapshots at A and B are still valid. Only D is new. This is correct because EDIT creates a new commit in the chain — it doesn't modify existing commits retroactively.

In `commit()`, the wireup looks like:

```python
elif operation == CommitOperation.EDIT:
    parent_snapshot = self._cache_get(prev_head) if prev_head else None
    if parent_snapshot is not None:
        edit_row = self._commit_repo.get(info.commit_hash)
        if edit_row is not None:
            patched = self._patch_snapshot_for_edit(parent_snapshot, info.commit_hash, edit_row)
            if patched is not None:
                self._cache_put(info.commit_hash, patched)
```

If any step fails (missing row, target not found, etc.), we simply don't cache — the next `compile()` call will do a full recompile and populate the cache fresh. Graceful degradation, no errors.

### Annotate: SKIP Patching and Cache Clearing

Annotation patching in `_patch_snapshot_for_annotate()` handles two cases:

**SKIP priority:** Find the target commit in `commit_hashes`, remove the message, config, and hash at that index, recount tokens. The snapshot shrinks by one message. If the target isn't in the snapshot (already skipped or not in this chain), return the snapshot unchanged.

**NORMAL or PINNED on an included commit:** The message is already in the snapshot. For PINNED, there's nothing to change at the message level (priority is metadata, not content). Return the snapshot unchanged.

**NORMAL or PINNED on an excluded commit (un-skip):** The message was previously skipped and isn't in the snapshot. We don't have its content cached. Return `None` to signal a full recompile. This is the asymmetric fallback — rare enough that storing all skipped messages isn't worth the complexity.

The `annotate()` method's cache management is more aggressive than EDIT:

```python
# In annotate():
if isinstance(self._compiler, DefaultContextCompiler):
    current_head = self.head
    patched = None
    if current_head:
        snapshot = self._cache_get(current_head)
        if snapshot is not None:
            patched = self._patch_snapshot_for_annotate(snapshot, target_hash, priority)
    self._cache_clear()  # Clear ALL entries
    if patched is not None:
        self._cache_put(current_head, patched)  # Re-add patched current HEAD
```

Why clear everything? An annotation affects every snapshot that contains the annotated commit. If you have snapshots cached at heads A→B→C and A→B→C→D, and you annotate commit B with SKIP, *both* snapshots are now stale. Rather than scanning every cached snapshot to check whether it contains B, we clear the entire cache and re-add only the patched current HEAD. Simple, correct, and annotations are infrequent enough that the cost is negligible.

### verify_cache Oracle in compile()

The oracle check lives in `compile()`, right after a cache hit:

```python
cached = self._cache_get(current_head)
if cached is not None:
    result = self._snapshot_to_compiled(cached)
    if self._verify_cache:
        fresh = self._compiler.compile(self._tract_id, current_head)
        assert result.messages == fresh.messages, (
            f"Cache message mismatch: cached {len(result.messages)} msgs, "
            f"fresh {len(fresh.messages)} msgs"
        )
        assert result.token_count == fresh.token_count, (
            f"Cache token mismatch: cached {result.token_count}, "
            f"fresh {fresh.token_count}"
        )
    return result
```

When `verify_cache=True`, every cache hit triggers a full recompile from the database. The two results are compared for message list equality and token count equality. If they diverge, you get an `AssertionError` with diagnostic info about what went wrong. This doubles the compile cost, which is why it defaults to `False` — but in tests, it's invaluable for catching subtle patching bugs.

The flag is set at construction time via `Tract.open(verify_cache=True)` or `Tract.from_components(verify_cache=True)`. Tests that exercise cache patching use this flag to prove that the patched result is identical to what a fresh recompile would produce.

## How It Connects

### What Phase 1.4 Builds On

**Phase 1.1 (Compile Cache):** Phase 1.4 replaces the single-snapshot cache infrastructure that Phase 1.1 introduced. The core concept — cache a `CompileSnapshot` and extend it incrementally on APPEND — survives, but the implementation is completely different. The `_extend_snapshot_for_append()` method is simpler now because there's no aggregation tail to handle.

**Phase 1.3 (Generation Config):** The copy-on-output and copy-on-input patterns from Phase 1.3 carry directly into the LRU cache. `_snapshot_to_compiled()` still shallow-copies `generation_configs` dicts when converting from cached snapshot to user-facing `CompiledContext`. The EDIT patching logic respects the "edit-inherits-original" rule: if an EDIT commit doesn't specify a `generation_config`, the original commit's config is preserved in the patched snapshot.

### What Depends on Phase 1.4

**Phase 2 (Branching):** The LRU cache is designed with branching in mind. When you switch between branches, each branch's HEAD has its own cached snapshot. Without LRU, every branch switch would require a full recompile from the database. With 8 cache slots, you can bounce between several branches and hit cached snapshots each time.

**Future Compression:** The `commit_hashes` parallel tuple enables targeted compression — you can identify exactly which commits contribute to the compiled output and make informed decisions about what to compress or summarize.

**Client-Side Features:** `CompiledContext.commit_hashes` is now a public field. Clients can use it to trace each message back to its source commit, build UI features like "show commit for this message," or filter compiled output by commit metadata.

### What Would Break If This Changed

If you re-introduced aggregation, `commit_hashes` would no longer be parallel to `messages` (one message could span multiple commits). EDIT patching would need to handle the case where the target commit's content was merged into an adjacent message. The complexity isn't worth it — aggregation should stay removed.

If you changed the cache key from `head_hash` to something else (like a compound key including time-travel parameters), the APPEND extension logic would need updating — it currently assumes the parent snapshot is keyed by the previous HEAD hash.

If you made `CompileSnapshot` mutable (removing `frozen=True`), the copy-on-output pattern would become critical for correctness rather than just safety. Right now, the frozen dataclass prevents accidental mutation; `dataclasses.replace()` is the only way to create modified snapshots.

## Usage Examples

### Basic LRU Cache Behavior

```python
from tract import Tract

with Tract.open(":memory:") as t:
    # Build up a conversation
    t.commit(role="user", content="Hello")
    t.commit(role="assistant", content="Hi there!")
    t.commit(role="user", content="How are you?")

    # First compile: full recompile, populates cache
    ctx1 = t.compile()
    assert len(ctx1.messages) == 3
    assert len(ctx1.commit_hashes) == 3  # Parallel to messages

    # Second compile at same HEAD: cache hit, no DB access
    ctx2 = t.compile()
    assert ctx2.messages == ctx1.messages
    assert ctx2.commit_hashes == ctx1.commit_hashes
```

### EDIT Patching with Oracle Verification

```python
with Tract.open(":memory:", verify_cache=True) as t:
    c1 = t.commit(role="user", content="Draft message")
    t.commit(role="assistant", content="Response")

    # Populate the cache
    t.compile()

    # EDIT the first commit — patched in-memory, not recompiled from DB
    t.commit(
        role="user",
        content="Revised message",
        operation="edit",
        response_to=c1.commit_hash,
    )

    # compile() will:
    # 1. Find patched snapshot in cache (EDIT stored it)
    # 2. Because verify_cache=True, also do a full recompile
    # 3. Assert patched result == fresh result
    ctx = t.compile()
    assert ctx.messages[0].content == "Revised message"
```

### Annotate SKIP Removes a Message

```python
with Tract.open(":memory:") as t:
    c1 = t.commit(role="user", content="Keep this")
    c2 = t.commit(role="user", content="Hide this")
    t.commit(role="assistant", content="Response")

    ctx_before = t.compile()
    assert len(ctx_before.messages) == 3

    # SKIP annotation removes the message from compiled output
    t.annotate(c2.commit_hash, priority="skip")

    ctx_after = t.compile()
    assert len(ctx_after.messages) == 2
    assert ctx_after.messages[0].content == "Keep this"
    assert ctx_after.messages[1].content == "Response"
```

### commit_hashes Tracking Through APPEND

```python
with Tract.open(":memory:") as t:
    c1 = t.commit(role="user", content="First")
    c2 = t.commit(role="assistant", content="Second")
    c3 = t.commit(role="user", content="Third")

    ctx = t.compile()

    # commit_hashes[i] is the commit that produced messages[i]
    assert ctx.commit_hashes[0] == c1.commit_hash
    assert ctx.commit_hashes[1] == c2.commit_hash
    assert ctx.commit_hashes[2] == c3.commit_hash

    # Lengths always match
    assert len(ctx.commit_hashes) == len(ctx.messages) == 3
```

### Batch Clears the Cache

```python
with Tract.open(":memory:") as t:
    t.commit(role="user", content="Setup")
    t.compile()  # Populate cache

    # batch() clears the entire cache on entry
    with t.batch() as b:
        b.commit(role="assistant", content="Batch 1")
        b.commit(role="user", content="Batch 2")
    # Cache was cleared; next compile() does a full recompile

    ctx = t.compile()
    assert len(ctx.messages) == 3
```
