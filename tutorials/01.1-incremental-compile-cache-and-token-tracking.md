---
date: 2026-02-10
summary: Incremental compile cache for O(1) append-path compilation and two-tier token tracking with API-reported usage as source of truth.
---

# Phase 1.1: Incremental Compile Cache & Token Tracking

## The Problems

Phase 1 established the compile pipeline: walk the commit chain from HEAD to root, resolve edits, filter by priority, build messages, aggregate same-role runs, and count tokens. This works correctly but has two design issues that become significant at scale.

### Problem 1: O(n) Compile Latency

Every call to `repo.compile()` walks the entire commit chain. For an agent conversation with 200 turns, each compile reads all 200 commits from the database, resolves edits, filters priorities, builds messages, aggregates, and counts tokens -- even if only a single new APPEND commit was added since the last compile.

The current cache helps if you call `compile()` twice without committing, but any commit clears the entire cache. Since the typical pattern is commit-then-compile (add user message, compile for API call), the cache provides no benefit for the dominant use case.

**The insight:** For APPEND-only workflows (the most common case), the previous compilation result plus the new message is all you need. The edit map did not change. The priority map gained one default entry. The effective commits grew by one. Only the tail of the message list changed.

### Problem 2: Token Source of Truth

Phase 1 uses tiktoken exclusively for token counting. Tiktoken estimates are good for pre-call budget enforcement ("will this context fit?"), but they diverge from what the API actually reports. Different models have different tokenizers, special tokens, system overhead, and cached content accounting. The API response's `usage` field is the authoritative source.

The standard pattern (used by LangChain, Langfuse, and other LLM frameworks) is two-tier:
1. **Pre-call:** tiktoken estimate for budget enforcement
2. **Post-call:** API-reported usage as source of truth for billing and monitoring

Phase 1 has the pre-call tier. Phase 1.1 adds the post-call tier via `repo.record_usage()`.

## Design: CompileSnapshot

The key data structure is `CompileSnapshot` -- a frozen dataclass that stores the intermediate compilation state, not just the final result. This enables incremental extension for APPEND operations.

### What It Stores

```python
@dataclass(frozen=True)
class CompileSnapshot:
    head_hash: str                          # Which HEAD this was compiled from
    raw_messages: tuple[Message, ...]       # One Message per effective commit, before aggregation
    aggregated_messages: tuple[Message, ...]# After same-role aggregation (the final output)
    effective_hashes: frozenset[str]        # Which commit hashes contributed
    commit_count: int                       # Number of effective commits
    token_count: int                        # Token count of aggregated messages
    token_source: str                       # "tiktoken:o200k_base" or "api:512+128"
```

The important distinction is between `raw_messages` (one per effective commit) and `aggregated_messages` (after same-role merging). We need the pre-aggregation list to correctly handle tail aggregation when extending.

### Why Tuples and Frozensets

The snapshot is frozen (immutable). Extension creates a new snapshot; it never mutates the old one. This avoids a class of bugs where a cached reference is accidentally modified. Tuples and frozensets enforce this at the data structure level.

### Cache Ownership: Repo, Not Compiler

The snapshot cache lives in `Repo`, not in `DefaultContextCompiler`. Reasons:

1. **Repo knows about operations.** It sees `CommitOperation.APPEND` vs `CommitOperation.EDIT` and can decide fast-path vs full-recompile. The compiler only sees a chain of commits.
2. **Compiler stays stateless.** The compiler is a pure function: (chain data) -> CompiledContext. Custom compilers (via the `ContextCompiler` protocol) should not need to implement caching.
3. **Single invalidation point.** Repo already manages cache invalidation on `commit()` and `annotate()`. Adding snapshot logic here is natural.

## The Incremental Fast Path

### When It Applies

The fast path activates when ALL of these are true:
- A `CompileSnapshot` exists for the previous HEAD
- The new commit is `CommitOperation.APPEND` (not EDIT)
- No time-travel parameters (`as_of`, `up_to`) are used
- `include_edit_annotations` is False
- The compiler is `DefaultContextCompiler` (not a custom compiler)

### How It Works

```
1. The snapshot has the compilation state as of the previous HEAD
2. The new APPEND commit is loaded (its blob, content type, role mapping)
3. A new Message is built from the commit
4. The raw_messages tuple is extended by one
5. Tail aggregation check:
   - If the new message's role matches the last aggregated message's role:
     merge them (concatenate content with "\n\n")
   - Otherwise: append as a new entry
6. Token recount on the full aggregated message list
7. A new CompileSnapshot is created with the updated state
```

Step 6 deserves attention: we do a full token recount on the aggregated list, not an incremental estimate. The aggregated message list is typically 10-100 items (due to same-role merging). Counting tokens on this list takes microseconds. The complexity of incremental token math (adjusting for aggregation boundary changes, per-message overhead, etc.) is not justified.

### When It Falls Back

These operations invalidate the snapshot (set it to `None`), forcing a full recompile on the next `compile()` call:

| Trigger | Why |
|---------|-----|
| EDIT commit | Changes the edit map -- a previous message's content changes retroactively |
| `annotate()` | Changes the priority map -- a commit may be SKIPped or un-SKIPped |
| `batch()` entry | Multiple commits happen without persistence; snapshot could reference uncommitted data |
| Time-travel params | `as_of` / `up_to` change which commits are visible |
| `include_edit_annotations=True` | Adds "[edited]" markers not present in cached state |
| Custom compiler | The incremental logic assumes DefaultContextCompiler's message-building rules |

After a full recompile, the snapshot is rebuilt from the full result, so subsequent APPEND commits can use the fast path again.

### Batch Interaction

Inside `repo.batch()`, the snapshot is invalidated on entry and not rebuilt during the batch. After the batch exits successfully, the next `compile()` does a full recompile and rebuilds the snapshot. This is the simplest approach and avoids rollback complexity (if the batch fails, no stale snapshot exists).

## Design: Two-Tier Token Tracking

### The API

```python
# Pre-call: tiktoken estimate (automatic)
context = repo.compile()
print(context.token_count)    # tiktoken estimate
print(context.token_source)   # "tiktoken:o200k_base"

# Post-call: record API-reported usage
response = openai_client.chat.completions.create(messages=context.messages)
updated = repo.record_usage(response.usage)  # or pass a dict
print(updated.token_count)    # API-reported prompt_tokens
print(updated.token_source)   # "api:512+128"
```

### record_usage()

Accepts either a `TokenUsage` dataclass or a raw dict. When given a dict, it normalizes across provider formats:

- **OpenAI format:** `{prompt_tokens, completion_tokens, total_tokens}`
- **Anthropic format:** `{input_tokens, output_tokens}`

The method updates the cached snapshot's token count and source, then returns an updated `CompiledContext`. It does NOT persist to the database -- this is in-memory only. Persistence is a future concern (Phase 2+ log command could show token history).

### Token Source String

The `token_source` field on `CompiledContext` is a string that disambiguates the source:

| Pattern | Meaning |
|---------|---------|
| `"tiktoken:o200k_base"` | Estimated by tiktoken using the o200k_base encoding |
| `"api:512+128"` | API reported 512 prompt tokens + 128 completion tokens |
| `""` | Unknown (e.g., NullTokenCounter) |

### Design Choice: Why Not Persist?

API-reported token usage is transient per-session. The user calls `compile()` to get a context, sends it to the API, gets usage back, and records it. The next compile call (after a new commit) will produce a new tiktoken estimate anyway. Persisting usage data adds schema complexity for minimal value at this stage.

## Connection to the Bigger Picture

### What This Enables

1. **Efficient agent loops.** An agent that does commit-compile-call-commit-compile-call benefits from O(1) incremental compilation instead of O(n) full chain walks.
2. **Accurate cost tracking.** Users can feed API actuals back into Trace, making token counts meaningful for billing and monitoring.
3. **Future compression decisions.** Phase 4 (compression) needs accurate token counts to decide when to compress. API actuals are more reliable than estimates for this.

### What Depends on This

- Phase 2 (`status` command) will show token budget usage. Using API actuals when available makes this more accurate.
- Phase 4 (compression) needs reliable token counts to trigger compression at the right threshold.

### What This Depends On

- Phase 1 compiler (`DefaultContextCompiler`) and its internal methods for message building, aggregation, and role mapping.
- Phase 1 `Repo` class structure and cache management patterns.

## Key Tradeoffs

| Decision | Alternative | Why This Way |
|----------|------------|--------------|
| Full token recount on extend | Incremental token math | Aggregated list is small (10-100 items); simplicity wins |
| Invalidate on EDIT/annotate | Selective EDIT fast path | Correctness over speed; EDITs are rare compared to APPENDs |
| Invalidate on batch entry | Track batch extensions, rollback on failure | Batch is for atomicity, not performance; one full recompile after batch is fine |
| In-memory only record_usage() | Persist to database | Adds schema complexity for minimal value now; revisit in Phase 2+ |
| Cache only default compiler | Cache all compilers | Custom compilers have unknown message-building rules; can't safely extend |
| Repo owns cache, not compiler | Compiler owns cache | Repo knows about operations; compiler stays stateless and composable |
