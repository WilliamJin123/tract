---
phase: 01.1-compile-cache-token-tracking
plan: 02
type: tdd
wave: 2
depends_on: ["01.1-01"]
files_modified:
  - src/tract/repo.py
  - src/tract/protocols.py
  - src/tract/exceptions.py
  - src/tract/__init__.py
  - tests/test_repo.py
autonomous: true

must_haves:
  truths:
    - "User can call repo.record_usage() with an OpenAI-format dict and get updated CompiledContext with API-reported token count"
    - "User can call repo.record_usage() with an Anthropic-format dict and get updated CompiledContext with API-reported token count"
    - "User can call repo.record_usage() with a TokenUsage dataclass directly"
    - "token_source on the returned CompiledContext reflects 'api:N+M' format after record_usage()"
    - "Subsequent compile() after record_usage() (without new commits) returns the API-reported counts"
    - "record_usage() raises TraceError when no commits exist"
    - "record_usage() with unrecognized dict format raises ContentValidationError"
  artifacts:
    - path: "src/tract/repo.py"
      provides: "record_usage() method and _normalize_usage_dict() helper"
      contains: "def record_usage"
    - path: "tests/test_repo.py"
      provides: "Tests for record_usage() with various provider formats"
      contains: "test_record_usage"
  key_links:
    - from: "src/tract/repo.py"
      to: "src/tract/protocols.py"
      via: "TokenUsage and CompileSnapshot usage in record_usage()"
      pattern: "TokenUsage|CompileSnapshot"
---

<objective>
Implement the `repo.record_usage()` API for feeding API-reported token usage back into Trace, establishing the post-call tier of two-tier token tracking.

Purpose: tiktoken estimates diverge from API actuals. Users need a way to record the real token usage after an API call, and the CompiledContext should reflect which source the token count came from. This completes the two-tier pattern: tiktoken for pre-call budget checks, API actuals for post-call truth.

Output: `record_usage()` method on Repo, usage dict normalization (OpenAI + Anthropic formats), updated snapshot with API token counts, comprehensive tests.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01.1-compile-cache-token-tracking/01.1-RESEARCH.md
@.planning/phases/01.1-compile-cache-token-tracking/01.1-01-SUMMARY.md
@tutorials/01.1-incremental-compile-cache-and-token-tracking.md
@src/tract/protocols.py
@src/tract/repo.py
@src/tract/__init__.py
@src/tract/exceptions.py
@tests/test_repo.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement record_usage() and usage dict normalization with TDD</name>
  <files>
    src/tract/repo.py
    src/tract/__init__.py
    tests/test_repo.py
  </files>
  <action>
    **RED phase -- Write failing tests first:**

    Add a new test class `TestRecordUsage` in `tests/test_repo.py`:

    1. `test_record_usage_openai_dict` -- Commit content, compile. Call `repo.record_usage({"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150})`. Assert returned CompiledContext has `token_count=100`, `token_source="api:100+50"`.

    2. `test_record_usage_anthropic_dict` -- Commit content, compile. Call `repo.record_usage({"input_tokens": 200, "output_tokens": 80})`. Assert returned CompiledContext has `token_count=200`, `token_source="api:200+80"`.

    3. `test_record_usage_token_usage_dataclass` -- Commit content, compile. Call `repo.record_usage(TokenUsage(prompt_tokens=300, completion_tokens=100, total_tokens=400))`. Assert returned CompiledContext has `token_count=300`, `token_source="api:300+100"`.

    4. `test_record_usage_updates_snapshot` -- Commit content, compile (snapshot populated with tiktoken counts). Call record_usage(). Then compile() again (without new commits). Assert the cached compile result still has the API-reported token count and source.

    5. `test_record_usage_no_commits_raises` -- Open empty repo. Call `repo.record_usage({"prompt_tokens": 100, ...})`. Assert raises `TraceError("Cannot record usage: no commits exist")`.

    6. `test_record_usage_unrecognized_format_raises` -- Commit content, compile. Call `repo.record_usage({"foo": 42})`. Assert raises `ContentValidationError`.

    7. `test_record_usage_no_prior_compile` -- Commit content (but do NOT compile). Call `repo.record_usage({"prompt_tokens": 100, ...})`. Assert it still works -- it should compile first (or return a result with the API counts). The behavior: if no snapshot exists, record_usage triggers a compile to build one, then updates it.

    8. `test_record_usage_with_head_hash` -- Commit A, B, C. Compile. Call `repo.record_usage({"prompt_tokens": 500, "completion_tokens": 200, "total_tokens": 700}, head_hash=C.commit_hash)`. Assert success. Then call with `head_hash="nonexistent"`. Assert raises `TraceError`.

    9. `test_token_source_reflects_api_after_record` -- Commit, compile, assert `token_source` starts with "tiktoken:". Call record_usage(). Assert `token_source` starts with "api:". Commit another APPEND, compile. Assert `token_source` is back to "tiktoken:" (new compile, no record_usage yet).

    **GREEN phase -- Implement record_usage():**

    Add to `Repo`:

    ```python
    def record_usage(
        self,
        usage: TokenUsage | dict,
        *,
        head_hash: str | None = None,
    ) -> CompiledContext:
        """Record API-reported token usage, updating cached compilation.

        Accepts either a TokenUsage dataclass or a provider-specific dict.
        Supported dict formats:
        - OpenAI: {prompt_tokens, completion_tokens, total_tokens}
        - Anthropic: {input_tokens, output_tokens}

        Args:
            usage: TokenUsage or dict with token counts.
            head_hash: Associate with a specific HEAD. Defaults to current HEAD.

        Returns:
            Updated CompiledContext with API-reported token count.

        Raises:
            TraceError: If no commits exist or head_hash doesn't match.
            ContentValidationError: If dict format is unrecognized.
        """
        from tract.protocols import TokenUsage as TU

        if isinstance(usage, dict):
            usage = self._normalize_usage_dict(usage)
        elif not isinstance(usage, TU):
            raise ContentValidationError(
                f"Expected TokenUsage or dict, got {type(usage).__name__}"
            )

        target_hash = head_hash or self.head
        if target_hash is None:
            raise TraceError("Cannot record usage: no commits exist")

        # If no snapshot, do a compile first to populate it
        if self._compile_snapshot is None or self._compile_snapshot.head_hash != target_hash:
            if head_hash is not None and head_hash != self.head:
                raise TraceError(
                    f"Cannot record usage: head_hash {head_hash} "
                    f"does not match current HEAD {self.head}"
                )
            # Trigger full compile to build snapshot
            self.compile()

        # Update snapshot with API-reported counts
        if self._compile_snapshot is not None and self._compile_snapshot.head_hash == target_hash:
            token_source = f"api:{usage.prompt_tokens}+{usage.completion_tokens}"
            self._compile_snapshot = CompileSnapshot(
                head_hash=self._compile_snapshot.head_hash,
                raw_messages=self._compile_snapshot.raw_messages,
                aggregated_messages=self._compile_snapshot.aggregated_messages,
                effective_hashes=self._compile_snapshot.effective_hashes,
                commit_count=self._compile_snapshot.commit_count,
                token_count=usage.prompt_tokens,
                token_source=token_source,
            )
            return self._snapshot_to_compiled(self._compile_snapshot)

        # Fallback: return result without snapshot update
        token_source = f"api:{usage.prompt_tokens}+{usage.completion_tokens}"
        return CompiledContext(
            messages=[],
            token_count=usage.prompt_tokens,
            commit_count=0,
            token_source=token_source,
        )

    def _normalize_usage_dict(self, usage_dict: dict) -> TokenUsage:
        """Normalize provider-specific usage dicts to TokenUsage.

        Supports:
        - OpenAI: {prompt_tokens, completion_tokens, total_tokens}
        - Anthropic: {input_tokens, output_tokens}

        Raises:
            ContentValidationError: If format not recognized.
        """
        from tract.protocols import TokenUsage as TU

        if "prompt_tokens" in usage_dict:
            return TU(
                prompt_tokens=usage_dict.get("prompt_tokens", 0),
                completion_tokens=usage_dict.get("completion_tokens", 0),
                total_tokens=usage_dict.get("total_tokens", 0),
            )
        elif "input_tokens" in usage_dict:
            input_t = usage_dict.get("input_tokens", 0)
            output_t = usage_dict.get("output_tokens", 0)
            return TU(
                prompt_tokens=input_t,
                completion_tokens=output_t,
                total_tokens=input_t + output_t,
            )
        else:
            raise ContentValidationError(
                f"Unrecognized usage dict format. "
                f"Expected 'prompt_tokens' (OpenAI) or 'input_tokens' (Anthropic). "
                f"Got keys: {list(usage_dict.keys())}"
            )
    ```

    Import `TokenUsage` and `CompileSnapshot` from `tract.protocols` (CompileSnapshot should already be imported from Plan 01).
    Import `TraceError` and `ContentValidationError` from `tract.exceptions`.

    Ensure `record_usage` is accessible as `repo.record_usage()` (no __init__.py changes needed beyond what Plan 01 did, since it's a method on Repo).
  </action>
  <verify>
    Run: `python -m pytest tests/test_repo.py::TestRecordUsage -v`
    All 9 record_usage tests pass.

    Run: `python -m pytest tests/ -v`
    Full suite passes with no regressions.
  </verify>
  <done>
    record_usage() works with OpenAI dicts, Anthropic dicts, and TokenUsage dataclass. Token source correctly flips from "tiktoken:*" to "api:N+M". Snapshot is updated. Errors raised for no commits and unrecognized formats. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: End-to-end two-tier token tracking integration tests</name>
  <files>
    tests/test_repo.py
  </files>
  <action>
    Add a test class `TestTwoTierTokenTracking` in `tests/test_repo.py` that tests the full workflow:

    1. `test_full_workflow_tiktoken_then_api` -- Simulates a real agent loop:
       ```python
       repo = Repo.open()
       repo.commit(InstructionContent(text="System prompt"))
       repo.commit(DialogueContent(role="user", text="Hello"))

       # Pre-call: tiktoken estimate
       ctx = repo.compile()
       assert ctx.token_source.startswith("tiktoken:")
       assert ctx.token_count > 0
       tiktoken_count = ctx.token_count

       # Simulate API call and record usage
       api_usage = {"prompt_tokens": tiktoken_count + 5, "completion_tokens": 42, "total_tokens": tiktoken_count + 47}
       updated = repo.record_usage(api_usage)
       assert updated.token_source == f"api:{tiktoken_count + 5}+42"
       assert updated.token_count == tiktoken_count + 5

       # Cached compile returns API counts
       cached = repo.compile()
       assert cached.token_source == updated.token_source
       assert cached.token_count == updated.token_count

       # New commit resets to tiktoken
       repo.commit(DialogueContent(role="assistant", text="Hi there!"))
       fresh = repo.compile()
       assert fresh.token_source.startswith("tiktoken:")
       ```

    2. `test_record_usage_survives_append_incremental` -- Record usage, then APPEND one more commit. The incremental extension should use tiktoken for the new count (not carry forward the old API count as a base):
       ```python
       repo.commit(InstructionContent(text="System"))
       ctx = repo.compile()
       repo.record_usage({"prompt_tokens": 999, "completion_tokens": 1, "total_tokens": 1000})

       repo.commit(DialogueContent(role="user", text="New message"))
       ctx2 = repo.compile()
       # The incremental extension recounted tokens with tiktoken
       assert ctx2.token_source.startswith("tiktoken:")
       assert ctx2.token_count != 999  # Not carrying forward the old API count
       ```

    3. `test_multiple_provider_formats_in_sequence` -- Record OpenAI usage, then record Anthropic usage (simulating a multi-provider workflow). Assert each updates correctly.

    4. `test_token_source_string_format` -- Verify exact string format: `"tiktoken:{encoding}"` and `"api:{prompt}+{completion}"`. Test edge case where prompt_tokens=0 and completion_tokens=0 -> `"api:0+0"`.
  </action>
  <verify>
    Run: `python -m pytest tests/test_repo.py::TestTwoTierTokenTracking -v`
    All 4 integration tests pass.

    Run: `python -m pytest tests/ -v`
    Full suite passes (all existing + Plan 01 + Plan 02 tests).
  </verify>
  <done>
    Two-tier token tracking works end-to-end. tiktoken is the pre-call estimator. API actuals are the post-call source of truth via record_usage(). Token source string accurately reflects the provenance. Full test suite passes.
  </done>
</task>

</tasks>

<verification>
Run the complete test suite:

```bash
python -m pytest tests/ -v
```

Expected: All existing 200+ tests + Plan 01 incremental cache tests + Plan 02 record_usage tests + Plan 02 integration tests pass.

Verify the phase success criteria from ROADMAP.md:

1. "Compiling after an APPEND commit reuses cached intermediate state" -- Verified by TestIncrementalCompileCache tests (Plan 01)
2. "EDIT and annotate operations trigger full cache invalidation" -- Verified by test_edit_invalidates_cache and test_annotate_invalidates_cache (Plan 01)
3. "User can feed API-reported token usage back into Trace via repo.record_usage()" -- Verified by TestRecordUsage tests (Plan 02)
4. "tiktoken remains the pre-call estimator; API actuals are post-call source of truth" -- Verified by test_full_workflow_tiktoken_then_api (Plan 02)
5. "CompiledContext.token_source accurately reflects whether counts came from tiktoken or API" -- Verified by test_token_source_reflects_api_after_record and test_token_source_string_format (Plan 02)
</verification>

<success_criteria>
1. record_usage() accepts TokenUsage, OpenAI dict, and Anthropic dict
2. Token source flips from "tiktoken:*" to "api:N+M" after record_usage()
3. Subsequent compile() (no new commits) returns API-reported counts
4. New commit after record_usage() resets to tiktoken counts
5. No-commits raises TraceError, bad format raises ContentValidationError
6. All 200+ existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/01.1-compile-cache-token-tracking/01.1-02-SUMMARY.md`
</output>
