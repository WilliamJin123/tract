# Phase 1.1: Incremental Compile Cache & Token Tracking - Research

**Researched:** 2026-02-10
**Domain:** Incremental caching for append-only compilation + two-tier token tracking (estimate vs API-actual)
**Confidence:** HIGH (internal codebase analysis) / MEDIUM (external API formats)

## Summary

Phase 1.1 addresses two design issues in the Phase 1 foundations:

1. **Compile latency**: The current `DefaultContextCompiler.compile()` walks the entire commit chain on every call (O(n) where n = chain length). For append-only workflows (the dominant use case), this is wasteful -- the previous compilation result plus the new message is all that's needed. An incremental cache stores the intermediate compilation state and extends it on APPEND, falling back to full recompilation on EDIT or annotate operations.

2. **Token source of truth**: Phase 1 uses tiktoken exclusively for token counting. But tiktoken estimates diverge from API-reported actuals (different model-specific tokenizers, special tokens, system overhead). The standard pattern is: use tiktoken as a pre-call estimator for budget enforcement, then accept API-reported usage as the post-call source of truth. This requires a `record_usage()` API and a way to surface which source was used.

**Primary recommendation:** Implement a snapshot-based incremental cache in the compiler (not in Repo), keyed by head_hash, storing all intermediate state needed to extend the compilation. Use a simple `repo.record_usage()` method that updates the most recent CompiledContext with API-reported token counts, flipping `token_source` from "tiktoken:*" to "api:*".

## Standard Stack

### Core

No new external libraries needed. This phase is entirely internal architecture work.

| Component | Location | Purpose | Why Standard |
|-----------|----------|---------|--------------|
| `dataclasses` | stdlib | CompileSnapshot frozen dataclass | Immutable intermediate state |
| `copy` | stdlib | Shallow copy of message lists | Avoid mutating cached state |
| `typing.NamedTuple` or `dataclass` | stdlib | TokenUsage already defined | Reuse existing protocol |
| tiktoken | existing dep | Pre-call token estimation | Already integrated |

### Supporting

No new dependencies. All work is in `src/tract/engine/compiler.py`, `src/tract/repo.py`, and `src/tract/protocols.py`.

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| In-memory snapshot cache | SQLite-backed cache table | Adds schema complexity, no benefit for in-process SDK |
| Frozen dataclass snapshot | Plain dict | Dict loses type safety; dataclass is trivial overhead |
| Single-level cache (head_hash -> result) | Multi-level (branch -> head -> result) | Multi-level premature; branches not implemented yet |
| Invalidate on any mutation | Selective invalidation per operation type | Selective is more complex but allows APPEND fast path |

## Architecture Patterns

### Current Architecture (What Exists)

```
src/tract/
  repo.py              # Repo class, owns _compile_cache: dict[str, CompiledContext]
  engine/
    compiler.py         # DefaultContextCompiler.compile() - full chain walk every time
    tokens.py           # TiktokenCounter.count_text/count_messages
  protocols.py          # CompiledContext(messages, token_count, commit_count, token_source)
                        # TokenUsage(prompt_tokens, completion_tokens, total_tokens)
                        # TokenUsageExtractor protocol (defined but unused)
```

### Current Compile Flow (The Problem)

```
repo.compile()
  -> check cache: _compile_cache[head_hash]?
  -> miss: compiler.compile(repo_id, head_hash)
    -> _walk_chain(head_hash)           # O(n) DB reads, head->root
    -> reverse to root->head order
    -> _build_edit_map(commits)         # O(n) scan for EDITs
    -> _build_priority_map(commits)     # 1 batch DB query + O(n) scan
    -> _build_effective_commits(...)    # O(n) filter
    -> _build_messages(...)             # O(n) blob lookups
    -> _aggregate_messages(...)         # O(n) scan
    -> count_messages(...)              # O(n) tiktoken encoding
  -> store in cache: _compile_cache[head_hash] = result
```

Every compile is O(n) in the chain length. The cache only helps if you call compile() twice without committing. After any commit, the cache is cleared entirely.

### Recommended Architecture: Incremental Compile Cache

#### Pattern 1: CompileSnapshot (Intermediate State Cache)

**What:** Store not just the final CompiledContext, but also the intermediate state needed to extend the compilation incrementally.

**When to use:** After any APPEND-only commit (the common case), extend the cached state rather than recompiling from scratch.

**Key insight:** For an APPEND commit with no EDITs or annotation changes:
- The edit_map doesn't change (new commit has no reply_to)
- The priority_map gains one entry (the new commit's default priority)
- The effective_commits list gains one entry (the new commit)
- The messages list gains one message (or extends the last if same-role aggregation applies)
- The token count needs to be recomputed on the full message list (or incrementally updated)

**Data structure:**

```python
@dataclass(frozen=True)
class CompileSnapshot:
    """Intermediate compilation state enabling incremental extension."""
    head_hash: str
    messages: list[Message]           # Pre-aggregation message list
    aggregated_messages: list[Message] # Post-aggregation (the final output)
    effective_hashes: set[str]        # Hashes of commits that contributed
    edit_targets: set[str]            # Hashes that have been edited (reply_to targets)
    last_commit_operation: str        # "append" or "edit"
    last_commit_hash: str             # Hash of the commit that produced this snapshot
    token_count: int                  # Token count of aggregated_messages
    token_source: str                 # Source identifier
```

**Extension flow (APPEND fast path):**

```
repo.commit(content, operation=APPEND)
  -> cache has snapshot for previous_head_hash
  -> new commit is APPEND, no reply_to
  -> load blob for new commit
  -> map to Message
  -> extend pre-aggregation messages list
  -> re-aggregate only the tail (check if last message has same role)
  -> recount tokens (or incrementally add new message tokens)
  -> create new CompileSnapshot with updated head_hash
  -> return new CompiledContext from snapshot
```

**Invalidation triggers (full recompile):**

```
1. EDIT commit (changes edit_map, may change effective_commits)
2. annotate() call (changes priority_map, may exclude/include commits)
3. Time-travel parameters (as_of, up_to) -- always full recompile
4. Cache miss (first compile, or after restart)
```

#### Pattern 2: Cache Ownership (Compiler vs Repo)

**What:** The incremental cache should live in the Repo, not in the DefaultContextCompiler.

**Why:**
- Repo already owns `_compile_cache` and manages invalidation on commit/annotate
- Repo knows about operations (APPEND vs EDIT) and can decide fast-path vs full recompile
- The compiler should remain stateless -- it's a pure function from (chain data) -> CompiledContext
- Custom compilers (via ContextCompiler protocol) don't need to implement caching

**Design:**
- Repo stores `_compile_snapshot: CompileSnapshot | None` instead of `_compile_cache: dict[str, CompiledContext]`
- On APPEND: extend snapshot
- On EDIT/annotate: set snapshot to None (trigger full recompile)
- Compiler.compile() remains unchanged (always full recompile)
- Repo adds a new method to build/extend snapshot that delegates to compiler for full recompile, but does incremental extension for APPEND

**Alternative:** Cache in compiler. But this couples the compiler to operation semantics, which violates the stateless design. The planner should use the Repo-owned pattern.

#### Pattern 3: Token Recount Strategy

**What:** After incremental extension, how to compute the new token_count.

**Options:**
1. **Full recount**: count_messages() on the entire aggregated list. O(n) in messages but n is small (messages, not commits). Simple and correct.
2. **Incremental estimate**: track per-message token counts, add the new message's count, adjust for aggregation. Complex and fragile (aggregation changes token count due to separator tokens, per-message overhead).

**Recommendation:** Full recount on the aggregated message list. The message list is typically 10-100 messages even for long conversations (due to same-role aggregation). count_messages() is fast (microseconds per message with tiktoken). The complexity of incremental token counting isn't justified.

### Anti-Patterns to Avoid

- **Caching inside the compiler**: Compiler should remain a pure function. Cache belongs in Repo.
- **Invalidating on every commit**: Only EDIT and annotate need full invalidation. APPEND is the fast path.
- **Storing the full chain in the snapshot**: The snapshot stores the result, not the inputs. Chain data stays in the database.
- **Optimizing token recount**: Recounting a small message list is negligible. Don't add complexity for microsecond gains.
- **Making snapshot mutable**: Snapshot should be frozen/immutable. Create new instances on extension.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Token counting from API response | Custom parser per provider | Existing `TokenUsageExtractor` protocol + thin wrappers | Protocol already defined in protocols.py; implementations are trivial |
| Message aggregation | Duplicate logic in incremental path | Reuse `_aggregate_messages()` from compiler | Same aggregation rules must apply; don't duplicate |
| Snapshot serialization | JSON/pickle for persistence | Don't persist snapshots at all | In-memory only; rebuilt on Repo.open() |

**Key insight:** The incremental cache is a performance optimization that lives entirely in memory. Don't persist it, don't serialize it, don't make it more complex than it needs to be. If the cache is missing, fall back to full compile (which always works).

## Common Pitfalls

### Pitfall 1: Aggregation Boundary Mismatch

**What goes wrong:** When extending the message list incrementally, the new message might need to aggregate with the previous last message (same role). If you just append, you get duplicate content or wrong message boundaries.

**Why it happens:** Same-role aggregation is a global pass in the current compiler. Incremental extension must handle the "tail aggregation" case.

**How to avoid:**
- Check if the new message's role matches the last aggregated message's role
- If yes: create a new aggregated message combining the two, replace the last
- If no: simply append
- This is a 5-line check, not a full re-aggregation

**Warning signs:** Test with consecutive same-role APPEND commits. If aggregation breaks, the message count will differ between full-compile and incremental-extend.

### Pitfall 2: Cache Staleness After Annotation

**What goes wrong:** User annotates a commit with SKIP, but the cached snapshot still includes that commit's message. Next compile returns stale data.

**Why it happens:** `annotate()` currently calls `_compile_cache.clear()` but if the new snapshot-based cache isn't properly invalidated on annotate, stale data persists.

**How to avoid:** `repo.annotate()` must set `_compile_snapshot = None`. This is already the pattern (current code clears the dict). Just ensure the new code does the equivalent.

**Warning signs:** Test: commit 3 items, compile, annotate middle one with SKIP, compile again. If the second compile shows 3 messages instead of 2, the cache wasn't invalidated.

### Pitfall 3: Token Source Confusion

**What goes wrong:** User calls `record_usage()` but the compiled context they're looking at was from a previous compile call. The token_source gets confused about what it refers to.

**Why it happens:** `record_usage()` updates the "latest" compiled context, but the user might have compiled multiple times or compiled with different parameters.

**How to avoid:**
- `record_usage()` should take an explicit compiled context or head_hash to associate the usage with
- OR: make it update the most recent unfiltered compile result (simple, matches the common workflow)
- Document clearly: record_usage() applies to the last compile() result

**Warning signs:** Test with compile() -> record_usage() -> compile() again. Second compile should still have API-reported tokens if head hasn't changed.

### Pitfall 4: EDIT Commit Incorrectly Taking Fast Path

**What goes wrong:** An EDIT commit looks like it could extend the snapshot (it's just a new commit), but it actually changes the edit_map which retroactively changes a previous message.

**Why it happens:** Optimization too aggressive -- treating EDIT the same as APPEND.

**How to avoid:** Check `operation` in `repo.commit()`. If EDIT, invalidate snapshot (set to None). If APPEND, extend. This is a clear, unambiguous check.

**Warning signs:** Test: commit A (user), commit B (edit of A), compile. If compiled output shows original A content instead of B's, the EDIT fast path incorrectly reused the snapshot.

### Pitfall 5: Batch Context Manager and Cache Interaction

**What goes wrong:** Inside `repo.batch()`, multiple commits happen but session.commit is deferred. The cache snapshot gets extended multiple times, but if the batch fails, the snapshot has stale extensions that were never persisted.

**Why it happens:** batch() replaces session.commit with a noop, so intermediate snapshot extensions may reference uncommitted data.

**How to avoid:** Two options:
1. Disable incremental extension inside batch() -- just invalidate on batch exit and let the next compile do a full recompile. Simpler.
2. Track batch state and only finalize snapshot extensions on successful batch exit.

**Recommendation:** Option 1. Batch is for atomicity, not performance. After a batch of N commits, a single full compile is fine. The N+1 problem is solved by the cache being populated after the first compile post-batch.

### Pitfall 6: Custom Compiler and Cache Bypass

**What goes wrong:** User provides a custom `ContextCompiler` via `Repo.open(compiler=custom)`. The incremental cache extends messages using the wrong compilation logic (default compiler assumptions about role mapping, aggregation, etc.).

**Why it happens:** The incremental extension in Repo uses assumptions about how messages are built that may not hold for custom compilers.

**How to avoid:** When a custom compiler is provided, disable incremental caching entirely. Only use the incremental fast path when the compiler is `DefaultContextCompiler`. Detection: `isinstance(self._compiler, DefaultContextCompiler)`.

**Warning signs:** Test: provide a custom compiler, commit twice, verify compile output matches the custom compiler's logic, not the default compiler's.

## Code Examples

### Example 1: CompileSnapshot Dataclass

```python
from dataclasses import dataclass, field
from tract.protocols import Message

@dataclass(frozen=True)
class CompileSnapshot:
    """Cached intermediate compilation state for incremental extension."""
    head_hash: str
    # Pre-aggregation messages (one per effective commit)
    raw_messages: tuple[Message, ...]
    # Post-aggregation messages (consecutive same-role merged)
    aggregated_messages: tuple[Message, ...]
    # Effective commit hashes that contributed to this compilation
    effective_hashes: frozenset[str]
    # Commit count
    commit_count: int
    # Token count of aggregated output
    token_count: int
    # Token source string
    token_source: str
```

### Example 2: Incremental Extension in Repo

```python
def compile(self, *, as_of=None, up_to=None, include_edit_annotations=False):
    current_head = self.head
    if current_head is None:
        return CompiledContext(messages=[], token_count=0, commit_count=0, token_source="")

    # Time-travel always does full compile
    if as_of is not None or up_to is not None:
        return self._full_compile(current_head, as_of=as_of, up_to=up_to,
                                   include_edit_annotations=include_edit_annotations)

    # Cache hit: exact same head
    if self._compile_snapshot and self._compile_snapshot.head_hash == current_head:
        return self._snapshot_to_compiled(self._compile_snapshot)

    # Full compile (cache miss or invalidated)
    result = self._compiler.compile(self._repo_id, current_head,
                                     include_edit_annotations=include_edit_annotations)
    self._compile_snapshot = self._build_snapshot(current_head, result)
    return result
```

### Example 3: APPEND Fast Path

```python
def _extend_snapshot_for_append(self, new_commit_info, snapshot):
    """Extend existing snapshot for a single APPEND commit."""
    # Load the new commit's blob and build its message
    new_message = self._build_message_for_commit(new_commit_info)

    # Extend raw messages
    new_raw = snapshot.raw_messages + (new_message,)

    # Handle tail aggregation
    if snapshot.aggregated_messages and new_message.role == snapshot.aggregated_messages[-1].role:
        # Merge with last message
        last = snapshot.aggregated_messages[-1]
        merged = Message(
            role=last.role,
            content=last.content + "\n\n" + new_message.content,
            name=last.name,
        )
        new_aggregated = snapshot.aggregated_messages[:-1] + (merged,)
    else:
        new_aggregated = snapshot.aggregated_messages + (new_message,)

    # Recount tokens on aggregated messages
    messages_dicts = [{"role": m.role, "content": m.content} for m in new_aggregated]
    new_token_count = self._token_counter.count_messages(messages_dicts)

    return CompileSnapshot(
        head_hash=new_commit_info.commit_hash,
        raw_messages=new_raw,
        aggregated_messages=new_aggregated,
        effective_hashes=snapshot.effective_hashes | {new_commit_info.commit_hash},
        commit_count=snapshot.commit_count + 1,
        token_count=new_token_count,
        token_source=snapshot.token_source,
    )
```

### Example 4: record_usage() API

```python
def record_usage(
    self,
    usage: TokenUsage | dict,
    *,
    head_hash: str | None = None,
) -> CompiledContext:
    """Record API-reported token usage, updating the cached compilation.

    Args:
        usage: TokenUsage dataclass or dict with prompt_tokens/completion_tokens/total_tokens
               (OpenAI format) or input_tokens/output_tokens (Anthropic format).
        head_hash: Optional specific head to associate with. Defaults to current HEAD.

    Returns:
        Updated CompiledContext with API-reported token count.
    """
    if isinstance(usage, dict):
        usage = self._normalize_usage_dict(usage)

    target_hash = head_hash or self.head
    if target_hash is None:
        raise TraceError("Cannot record usage: no commits exist")

    # Update the snapshot with API-reported prompt tokens
    if self._compile_snapshot and self._compile_snapshot.head_hash == target_hash:
        updated = CompileSnapshot(
            head_hash=self._compile_snapshot.head_hash,
            raw_messages=self._compile_snapshot.raw_messages,
            aggregated_messages=self._compile_snapshot.aggregated_messages,
            effective_hashes=self._compile_snapshot.effective_hashes,
            commit_count=self._compile_snapshot.commit_count,
            token_count=usage.prompt_tokens,
            token_source=f"api:{usage.prompt_tokens}+{usage.completion_tokens}",
        )
        self._compile_snapshot = updated

    return CompiledContext(
        messages=list(self._compile_snapshot.aggregated_messages) if self._compile_snapshot else [],
        token_count=usage.prompt_tokens,
        commit_count=self._compile_snapshot.commit_count if self._compile_snapshot else 0,
        token_source=f"api:{usage.prompt_tokens}+{usage.completion_tokens}",
    )
```

### Example 5: Usage Dict Normalization

```python
def _normalize_usage_dict(self, usage_dict: dict) -> TokenUsage:
    """Normalize provider-specific usage dicts to TokenUsage.

    Supports:
    - OpenAI format: {prompt_tokens, completion_tokens, total_tokens}
    - Anthropic format: {input_tokens, output_tokens}
    """
    if "prompt_tokens" in usage_dict:
        # OpenAI format
        return TokenUsage(
            prompt_tokens=usage_dict.get("prompt_tokens", 0),
            completion_tokens=usage_dict.get("completion_tokens", 0),
            total_tokens=usage_dict.get("total_tokens", 0),
        )
    elif "input_tokens" in usage_dict:
        # Anthropic format
        input_t = usage_dict.get("input_tokens", 0)
        output_t = usage_dict.get("output_tokens", 0)
        return TokenUsage(
            prompt_tokens=input_t,
            completion_tokens=output_t,
            total_tokens=input_t + output_t,
        )
    else:
        raise ContentValidationError(
            f"Unrecognized usage dict format. Expected 'prompt_tokens' or 'input_tokens'. Got: {list(usage_dict.keys())}"
        )
```

## State of the Art

### LLM API Token Reporting Formats

| Provider | Response Field | Prompt Key | Completion Key | Total Key | Extra Fields |
|----------|---------------|------------|----------------|-----------|--------------|
| OpenAI | `usage` | `prompt_tokens` | `completion_tokens` | `total_tokens` | `prompt_tokens_details`, `completion_tokens_details` |
| Anthropic | `usage` | `input_tokens` | `output_tokens` | (computed) | `cache_creation_input_tokens`, `cache_read_input_tokens` |
| Google (Gemini) | `usageMetadata` | `promptTokenCount` | `candidatesTokenCount` | `totalTokenCount` | `cachedContentTokenCount` |

**Confidence:** MEDIUM -- based on current API documentation as of 2026-02-10. API formats may evolve.

### Two-Tier Token Pattern (Industry Standard)

The standard pattern used by LangChain, Langfuse, and other LLM frameworks:

1. **Pre-call estimate**: Use tiktoken (or model-specific tokenizer) to estimate prompt token count before calling the API. Used for budget enforcement and cost estimation.
2. **Post-call actual**: Use the `usage` field from the API response as the source of truth. Store this for billing, monitoring, and analytics.

This is exactly what Phase 1.1 implements. The `token_source` field on `CompiledContext` disambiguates which tier the count came from.

| Old Approach (Phase 1) | Current Approach (Phase 1.1) | Impact |
|------------------------|------------------------------|--------|
| tiktoken only | tiktoken (pre-call) + API actual (post-call) | Accurate billing/monitoring |
| `token_source: "tiktoken:o200k_base"` | `token_source: "tiktoken:o200k_base"` or `"api:512+128"` | Clear provenance |
| No record_usage() | `repo.record_usage(api_response["usage"])` | Simple API |

## Open Questions

### 1. Should record_usage() persist to database?

- **What we know:** The current design keeps compiled context in memory only. Token usage from API responses is transient per-session.
- **What's unclear:** Should API-reported token counts be persisted in the database for historical analytics? This would require a new table or extending CommitRow.
- **Recommendation:** Phase 1.1 keeps it in-memory only. Persistence is a Phase 2+ concern (log command could show token usage history). Keep it simple now.

### 2. Should the snapshot cache handle the `include_edit_annotations` parameter?

- **What we know:** `include_edit_annotations=True` adds " [edited]" markers to messages. This changes the compiled output.
- **What's unclear:** Should the snapshot cache key include this parameter, or should it always be `False` for the cache?
- **Recommendation:** Cache only the `include_edit_annotations=False` case (the default). When `True` is passed, do a full recompile. This simplifies the cache significantly. The annotation parameter is rarely used (debugging only).

### 3. How should Repo._build_message_for_commit() work?

- **What we know:** The current compiler loads blob data, parses JSON, maps to role, extracts text. For the incremental path, Repo needs to do the same thing for a single commit.
- **What's unclear:** Should Repo duplicate the compiler's message-building logic, or should the compiler expose a helper method?
- **Recommendation:** Extract the single-commit message-building logic from `DefaultContextCompiler` into a reusable method (e.g., `build_message_for_commit(commit_row)` or make the existing private methods callable). This avoids duplication. Alternatively, have Repo call the compiler's internal methods when it's a DefaultContextCompiler instance.

### 4. Token recount: full or incremental?

- **What we know:** Full recount on aggregated messages is O(m) where m = message count (typically 10-100). This is fast.
- **What's unclear:** For very long conversations (1000+ messages), could recount become a bottleneck?
- **Recommendation:** Start with full recount. Profile later. 1000 messages * ~100 tokens each = 100K tokens to encode with tiktoken, which takes ~10ms. Acceptable for v1.

### 5. Interaction with batch() context manager

- **What we know:** batch() defers session.commit(). Multiple commits happen without DB persistence.
- **What's unclear:** Should incremental cache extensions happen during batch, or only after?
- **Recommendation:** Invalidate cache on batch entry, don't extend during batch. Rebuild on first compile() after batch exits. Simplest approach, avoids rollback complexity.

## Sources

### Primary (HIGH confidence)
- Codebase analysis: `src/tract/engine/compiler.py` -- current compile flow, all 8 steps documented
- Codebase analysis: `src/tract/repo.py` -- current cache implementation, commit/annotate invalidation
- Codebase analysis: `src/tract/protocols.py` -- CompiledContext, TokenUsage, TokenUsageExtractor protocols
- Codebase analysis: `src/tract/engine/tokens.py` -- TiktokenCounter implementation
- Codebase analysis: `tests/test_engine/test_compiler.py` -- 26 tests covering all compile paths
- Codebase analysis: `tests/test_repo.py` -- 47 integration tests including cache invalidation

### Secondary (MEDIUM confidence)
- [Anthropic Messages API](https://platform.claude.com/docs/en/api/messages) -- usage field format: input_tokens, output_tokens, cache_creation_input_tokens, cache_read_input_tokens
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/completions) -- usage field format: prompt_tokens, completion_tokens, total_tokens
- [Langfuse Token Tracking](https://langfuse.com/docs/observability/features/token-and-cost-tracking) -- industry standard two-tier pattern

### Tertiary (LOW confidence)
- [LangChain Token Usage Tracking](https://python.langchain.com/docs/how_to/llm_token_usage_tracking/) -- URL now redirects; pattern confirmed by Langfuse docs
- General knowledge of incremental compilation patterns from compiler design

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- no new dependencies, purely internal architecture
- Architecture patterns: HIGH -- derived from direct codebase analysis; the incremental cache design follows directly from the existing compile flow
- Pitfalls: HIGH -- identified from code review; each pitfall maps to specific code paths
- API formats: MEDIUM -- based on current docs, may evolve
- Token recount performance: MEDIUM -- estimated, not benchmarked

**Research date:** 2026-02-10
**Valid until:** 2026-04-10 (stable internal architecture; API formats may shift)
