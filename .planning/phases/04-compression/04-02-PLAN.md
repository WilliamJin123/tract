---
phase: 04-compression
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/tract/operations/compression.py
  - src/tract/tract.py
  - tests/test_compression.py
autonomous: true

must_haves:
  truths:
    - "User can compress all NORMAL commits into summary commits via t.compress() with LLM client"
    - "User can compress a specific range via from_commit/to_commit or a commit list"
    - "PINNED commits survive compression verbatim -- their content is unchanged and position preserved"
    - "SKIP commits are ignored during compression (not summarized, not preserved)"
    - "User can provide manual content instead of LLM summarization via content= parameter"
    - "User can get a draft for review via auto_commit=False and approve with result.approve()"
    - "Compression creates a CompressionRecord with full provenance (sources + results)"
    - "Original commits remain in DB as unreachable after compression (non-destructive)"
    - "Compile cache is cleared after compression"
    - "compress() without LLM client or manual content raises CompressionError"
  artifacts:
    - path: "src/tract/operations/compression.py"
      provides: "compress_range() core operation"
      contains: "def compress_range"
    - path: "src/tract/tract.py"
      provides: "Tract.compress() and Tract.approve_compression() facade"
      contains: "def compress"
    - path: "tests/test_compression.py"
      provides: "Compression integration tests"
      min_lines: 200
  key_links:
    - from: "src/tract/tract.py"
      to: "src/tract/operations/compression.py"
      via: "Tract.compress() delegates to compress_range()"
      pattern: "compress_range"
    - from: "src/tract/operations/compression.py"
      to: "src/tract/engine/commit.py"
      via: "Summary commits created via CommitEngine.create_commit()"
      pattern: "commit_engine.create_commit"
    - from: "src/tract/operations/compression.py"
      to: "src/tract/storage/sqlite.py"
      via: "CompressionRecord saved via SqliteCompressionRepository"
      pattern: "compression_repo.save_record"
    - from: "src/tract/operations/compression.py"
      to: "src/tract/prompts/summarize.py"
      via: "Default prompt used for LLM summarization"
      pattern: "build_summarize_prompt"
    - from: "src/tract/operations/compression.py"
      to: "src/tract/llm/protocols.py"
      via: "LLM client used for summarization calls"
      pattern: "llm_client.chat"
---

<objective>
Implement the compression engine and Tract facade -- the core capability of Phase 4.

Purpose: Users need to compress commit chains into summaries to fit token budgets. This plan implements the three autonomy modes (autonomous LLM, collaborative review, manual content), the PINNED commit preservation logic, and the provenance recording. This is the primary deliverable for requirements COMP-01 and COMP-02.

Output: operations/compression.py with compress_range(), Tract.compress() and Tract.approve_compression() facade methods, and comprehensive integration tests. All 3 autonomy modes working.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-compression/04-CONTEXT.md
@.planning/phases/04-compression/04-RESEARCH.md
@.planning/phases/04-compression/04-01-SUMMARY.md

@src/tract/operations/merge.py
@src/tract/tract.py
@src/tract/llm/client.py
@src/tract/llm/protocols.py
@src/tract/engine/commit.py
@src/tract/models/compression.py
@src/tract/prompts/summarize.py
@src/tract/storage/sqlite.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: compress_range() operation + Tract facade</name>
  <files>
    src/tract/operations/compression.py
    src/tract/tract.py
  </files>
  <action>
    **operations/compression.py** -- Create new file following the operations/merge.py pattern:

    ```python
    # Imports: logging, json, uuid, datetime, TYPE_CHECKING
    # From tract: exceptions (CompressionError), models (CompressResult, PendingCompression),
    #   content (DialogueContent), prompts (DEFAULT_SUMMARIZE_SYSTEM, build_summarize_prompt),
    #   annotations (Priority), operations (row_to_info)
    ```

    **Helper functions:**

    1. `_resolve_commit_range(commit_repo, ref_repo, annotation_repo, tract_id, *, commits, from_commit, to_commit) -> list[CommitRow]`:
       - If `commits` provided: resolve each hash to CommitRow, return in chain order
       - If `from_commit/to_commit` provided: walk ancestors from HEAD, filter to range
       - If none provided (default): return ALL ancestors from HEAD (full first-parent chain)
       - NOTE: Compression operates on the first-parent chain. Commits reachable only through merge second-parents are not included in the range. This mirrors the compiler's own `_walk_chain()` pattern.
       - Validates: range is non-empty, all commits belong to tract_id

    2. `_classify_by_priority(range_commits, annotation_repo, preserve) -> tuple[list, list, list]`:
       - Uses `annotation_repo.batch_get_latest()` for efficient priority lookup
       - Classifies each commit as PINNED, NORMAL, or SKIP
       - `preserve` parameter: treat listed commit hashes as temporarily PINNED for this invocation
       - PINNED is sacred: no override can compress PINNED commits
       - Returns (pinned_commits, normal_commits, skip_commits) -- each is list of CommitRow

    3. `_partition_around_pinned(range_commits, pinned_hashes) -> list[list[CommitRow]]`:
       - Walks range_commits in order
       - PINNED commits act as boundaries between groups
       - Returns list of groups where each group is consecutive NORMAL commits
       - Example: [N, N, P, N, N, N, P, N] -> [[N,N], [N,N,N], [N]] with P's preserved separately

    4. `_build_messages_text(group, blob_repo) -> str`:
       - For each commit in the group, loads blob content and formats as text
       - Returns concatenated text for LLM summarization
       - Implements its own content-to-text conversion (do NOT import private helpers from merge.py)

    4b. `_reconstruct_content(commit_row, blob_repo, type_registry) -> BaseModel`:
       - Load blob via `blob_repo.get(commit_row.content_hash)`
       - Parse `payload_json` from the blob
       - Use `commit_row.content_type` to look up the correct Pydantic model class from `type_registry`
       - Instantiate and return the content model
       - Used by `_commit_compression()` to re-create PINNED commits with correct content models for `create_commit()`

    5. `_summarize_group(messages_text, llm_client, token_counter, *, target_tokens, instructions, system_prompt) -> str`:
       - Builds prompt using `build_summarize_prompt()` from prompts module
       - If `system_prompt` is provided, use it; otherwise use DEFAULT_SUMMARIZE_SYSTEM
       - If `instructions` is provided, pass to build_summarize_prompt()
       - Calls `llm_client.chat()` with system + user messages
       - Extracts content text from response (response["choices"][0]["message"]["content"])
       - Returns summary text string
       - Raises CompressionError if LLM returns empty response

    **Core function:**

    6. `compress_range(tract_id, commit_repo, blob_repo, annotation_repo, ref_repo, commit_engine, token_counter, compression_repo, parent_repo, *, commits=None, from_commit=None, to_commit=None, target_tokens=None, preserve=None, auto_commit=True, llm_client=None, content=None, instructions=None, system_prompt=None) -> CompressResult | PendingCompression`:

       Flow:
       a. Resolve HEAD and current branch name
       b. Call `_resolve_commit_range()` to get the commit range
       c. Call `_classify_by_priority()` to separate PINNED/NORMAL/SKIP
       d. If no NORMAL commits: raise CompressionError("Nothing to compress -- all commits are pinned or skipped")
       e. Call `_partition_around_pinned()` to get groups

       f. Generate summaries based on mode:
          - **Manual mode** (`content is not None`): `summaries = [content]` (user provides text). Ignore LLM.
          - **LLM mode** (`llm_client is not None`):
            - For each group, call `_build_messages_text()` then `_summarize_group()`
            - Collect summaries list (one per group)
          - **Error**: Neither content nor llm_client -> raise CompressionError("No LLM client configured and no manual content provided. Call configure_llm() first or pass content='...'.")

       g. Calculate token counts:
          - `original_tokens = sum(c.token_count for c in normal_commits)`
          - `estimated_tokens` for each summary via `token_counter.count_text()`

       h. If `auto_commit=False` (collaborative mode):
          - Return PendingCompression with summaries, source_commits, preserved_commits, original_tokens, estimated_tokens
          - Set `_commit_fn` on PendingCompression (see below)

       i. If `auto_commit=True` (autonomous mode):
          - Call `_commit_compression()` to create summary commits and provenance

    7. `_commit_compression(tract_id, commit_repo, blob_repo, ref_repo, commit_engine, token_counter, compression_repo, summaries, range_commits, pinned_commits, normal_commits, original_tokens, target_tokens, instructions, branch_name) -> CompressResult`:

       Flow:
       a. Find the parent of the compressed range (the commit just before the first commit in range). If the first commit in range is the root commit, parent is None.
       a2. **CRITICAL: Reset branch ref to pre-range parent** via `ref_repo.update_head(tract_id, pre_range_parent_hash)` so that `create_commit()` chains from the correct starting point. If the range starts at the root commit (no pre-range parent), reset HEAD to None (clear it). This ensures the first `create_commit()` call reads the correct parent from HEAD.
       b. Generate `compression_id = uuid.uuid4().hex`
       c. Create summary commits in chain order, interleaving with PINNED:
          - Walk the original range order
          - When encountering a PINNED commit: use `_reconstruct_content()` to deserialize the original blob back into the correct Pydantic content model, then call `commit_engine.create_commit()` with the reconstructed content model. This creates a new commit with the same content but correct parent pointer in the new chain.
          - When encountering a group boundary: create a summary commit using `DialogueContent(role="assistant", text=summary_text)` via `commit_engine.create_commit()`
          - Each new commit's parent is the previous commit in the new chain (handled automatically by `create_commit()` reading HEAD)
          - The summary commit message = "Compressed {N} commits"
       d. After all commits, the branch ref already points to the new chain tip (last `create_commit()` updated HEAD).
       e. Save CompressionRecord:
          - `compression_repo.save_record(compression_id, tract_id, branch_name, datetime.now(), original_tokens, compressed_tokens, target_tokens, instructions)`
          - For each normal commit: `compression_repo.add_source(compression_id, commit_hash, position)`
          - For each summary commit: `compression_repo.add_result(compression_id, commit_hash, position)`
       f. Return CompressResult with all fields populated

       IMPORTANT: For PINNED commits in the range, create NEW commits with the same content blob but new parent pointers (to maintain chain integrity). The original PINNED commits become unreachable but their content is preserved via the new commits. This ensures the new chain is contiguous from pre-range parent to tip.

       IMPORTANT: Use `commit_engine.create_commit()` for each summary/pinned commit. This handles blob dedup, hash computation, and HEAD update. The HEAD will automatically advance with each commit. After all commits, the branch ref points to the last one.

    **tract.py** changes:

    1. Add `compression_repo` parameter to `__init__()`:
       - `self._compression_repo = compression_repo`

    2. Update `Tract.open()`:
       - After creating parent_repo, create: `compression_repo = SqliteCompressionRepository(session)`
       - Pass to constructor: `compression_repo=compression_repo`
       - Add import: `from tract.storage.sqlite import SqliteCompressionRepository`

    3. Update `Tract.from_components()`:
       - Add optional `compression_repo` parameter
       - If None, skip compression functionality (don't fail -- tests may not provide it)

    4. Add `Tract.compress()` method:
       ```python
       def compress(
           self,
           *,
           commits: list[str] | None = None,
           from_commit: str | None = None,
           to_commit: str | None = None,
           target_tokens: int | None = None,
           preserve: list[str] | None = None,
           auto_commit: bool = True,
           content: str | None = None,
           instructions: str | None = None,
           system_prompt: str | None = None,
       ) -> CompressResult | PendingCompression:
       ```
       - **Guard: Check for detached HEAD** -- `if self._ref_repo.is_detached(self._tract_id): raise DetachedHeadError()` (same pattern as commit/merge)
       - Lazy import: `from tract.operations.compression import compress_range`
       - Determine LLM client: `llm_client = getattr(self, "_llm_client", None)`
       - Call `compress_range()` with all repos and params (including `self._parent_repo`)
       - If result is PendingCompression: set `result._commit_fn = self._finalize_compression`
       - Call `self._session.commit()` (if auto_commit was True and result is CompressResult)
       - Clear compile cache: `self._cache.clear()`
       - Return result
       - NOTE: Session safety follows the deferred-commit pattern (same as `Tract.commit()` and `Tract.batch()`). If `compress_range()` raises, `session.commit()` is never reached, so flushed-but-uncommitted changes are discarded.
       - NOTE: When `content` is provided (manual mode), `instructions`, `system_prompt`, and `target_tokens` are ignored since no LLM is called.

    5. Add `Tract._finalize_compression(pending)` private method:
       - Called by PendingCompression.approve() via the _commit_fn closure
       - Calls `_commit_compression()` from operations/compression.py with the pending's summaries
       - Calls `self._session.commit()`
       - Clears compile cache
       - Returns CompressResult

    6. Add `Tract.approve_compression(pending)` public method (alternative to pending.approve()):
       - Delegates to `self._finalize_compression(pending)`
       - This follows the `commit_merge()` pattern from merge
  </action>
  <verify>
    Run `python -m pytest tests/ -x -q` -- all existing tests pass (no regressions from tract.py changes).
    Run `python -c "from tract.operations.compression import compress_range; print('OK')"` -- operation importable.
    Run `python -c "t = __import__('tract').Tract.open(); print(hasattr(t, 'compress'))"` -- compress method exists.
  </verify>
  <done>
    compress_range() operation with 7 helper functions. Tract.compress(), Tract.approve_compression(), and Tract._finalize_compression(). compression_repo wired into Tract.open() and __init__. All 3 autonomy modes functional. All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Compression integration tests</name>
  <files>
    tests/test_compression.py
  </files>
  <action>
    Create comprehensive test file covering all compression functionality. Use the same test patterns as test_merge.py and test_rebase.py (Tract.open() with :memory:, create commits, exercise operations).

    **Mock LLM client**: Create a `MockLLMClient` class in the test file:
    ```python
    class MockLLMClient:
        def __init__(self, responses=None):
            self.responses = responses or ["Previously in this conversation: summary text."]
            self._call_count = 0
        def chat(self, messages, **kwargs):
            text = self.responses[min(self._call_count, len(self.responses) - 1)]
            self._call_count += 1
            return {"choices": [{"message": {"content": text}}], "usage": {"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15}}
        def close(self): pass
    ```

    **Test categories:**

    1. Autonomous mode (~7 tests):
       - `test_compress_all_normal_commits`: Create 5 commits, configure_llm, compress(), verify CompressResult has summary_commits, source_commits, compression_ratio < 1.0, new_head != old_head
       - `test_compress_preserves_pinned`: Create 5 commits, pin commit 3, compress(), verify preserved_commits contains pinned hash, compiled output has pinned content verbatim
       - `test_compress_ignores_skip`: Create 5 commits, skip commit 2, compress(), verify skipped commit is NOT in source_commits
       - `test_compress_with_range`: Create 10 commits, compress(from_commit=h3, to_commit=h7), verify only h3-h7 compressed
       - `test_compress_with_commit_list`: Create 5 commits, compress(commits=[h1, h3, h5]), verify exactly those compressed
       - `test_compress_with_target_tokens`: compress(target_tokens=100), verify LLM was called with token target in prompt
       - `test_compress_with_instructions`: compress(instructions="focus on code"), verify LLM prompt includes instructions

    2. Collaborative mode (~4 tests):
       - `test_compress_auto_commit_false`: compress(auto_commit=False) returns PendingCompression with summaries
       - `test_pending_edit_summary`: compress(auto_commit=False), edit_summary(0, "new text"), approve(), verify new text in committed summary
       - `test_pending_approve`: compress(auto_commit=False), approve() returns CompressResult, verify commits created
       - `test_approve_compression_method`: Use t.approve_compression(pending) instead of pending.approve()

    3. Manual mode (~3 tests):
       - `test_compress_manual_content`: compress(content="My manual summary"), no LLM needed, verify summary commit has the provided text
       - `test_compress_manual_no_llm_required`: compress(content="...") works even without configure_llm()
       - `test_compress_manual_preserves_pinned`: compress(content="...") still preserves PINNED commits

    4. Provenance (~4 tests):
       - `test_compression_record_created`: After compress(), verify CompressionRecord exists in DB with correct source and result hashes
       - `test_original_commits_unreachable`: After compress(), verify original commits still in DB but not reachable from HEAD
       - `test_provenance_query_sources`: Query "what sources produced this summary?" via compression_repo.get_sources()
       - `test_provenance_query_results`: Query "what results came from this compression?" via compression_repo.get_results()

    5. Edge cases + errors (~5 tests):
       - `test_compress_no_commits_raises`: compress() on empty tract raises TraceError
       - `test_compress_no_llm_no_content_raises`: compress() without configure_llm() and without content= raises CompressionError
       - `test_compress_all_pinned_raises`: All commits pinned, compress() raises CompressionError (nothing to compress)
       - `test_compress_clears_cache`: compile() before compress, compile() after compress, verify different results
       - `test_compress_then_compile_coherent`: After compress(), compile() returns coherent messages including summaries and pinned content

    6. Multi-pinned interleaving (~2 tests):
       - `test_pinned_interleaving_order`: Create [c1, c2(pinned), c3, c4(pinned), c5], compress(), verify compiled output order = [summary_1, pinned_2, summary_2, pinned_4, summary_3]
       - `test_pinned_at_boundaries`: Pin first and last commit in range, verify they appear in correct positions

    Each test should:
    - Use `Tract.open()` with `:memory:` database
    - Create test commits using `t.commit(DialogueContent(role="user", text="..."))` or similar
    - Use MockLLMClient with `t.configure_llm()`
    - Assert on CompressResult fields, compiled output, and provenance records
  </action>
  <verify>
    Run `python -m pytest tests/test_compression.py -v` -- all ~25 new tests pass.
    Run `python -m pytest tests/ -x -q` -- all tests pass (existing + new, total ~535).
  </verify>
  <done>
    ~25 integration tests covering autonomous/collaborative/manual modes, PINNED preservation, provenance tracking, edge cases, and multi-pinned interleaving. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_compression.py -v` -- all ~25 new tests pass
2. `python -m pytest tests/ -x -q` -- all tests pass (~535 total)
3. Manual verification: Create 5 commits, pin one, compress(), verify:
   - CompressResult.preserved_commits contains pinned hash
   - CompressResult.source_commits contains normal hashes
   - CompressResult.compression_ratio < 1.0
   - Compiled output after compression includes summary and pinned content
4. Verify non-destructive: original commits still exist in DB after compression
5. Verify provenance: compression_repo.get_sources() and get_results() return correct data
</verification>

<success_criteria>
1. compress() with LLM client produces summary commits and CompressResult with all fields
2. PINNED commits survive verbatim in correct positions (COMP-02 complete)
3. SKIP commits are ignored during compression
4. compress(auto_commit=False) returns PendingCompression; approve() creates commits
5. compress(content="...") works without LLM (manual mode)
6. CompressionRecord provenance is created and queryable
7. Original commits remain in DB (non-destructive)
8. Compile cache cleared after compression
9. ~25 new tests; zero regressions on existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/04-compression/04-02-SUMMARY.md`
</output>
