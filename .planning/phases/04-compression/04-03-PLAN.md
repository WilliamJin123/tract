---
phase: 04-compression
plan: 03
type: execute
wave: 3
depends_on: ["04-02"]
files_modified:
  - src/tract/tract.py
  - src/tract/operations/compression.py
  - tests/test_reorder.py
  - tests/test_gc.py
autonomous: true

must_haves:
  truths:
    - "User can compile with a custom order via compile(order=[hash3, hash1, hash2]) and messages appear in that order"
    - "Structural safety checks warn when EDIT appears before its target in the reordered sequence"
    - "Structural safety checks warn when response_to chain is broken by reordering"
    - "compile(order=...) bypasses the compile cache (reordered compiles are special views)"
    - "User can run gc() to remove unreachable commits older than retention period"
    - "GC respects time-based retention: only removes orphans older than orphan_retention_days"
    - "GC respects compression archives: archived commits protected by default (archive_retention=None)"
    - "GC with archive_retention_days set can remove old archived commits"
    - "GC returns GCResult with commits_removed, blobs_removed, tokens_freed, archives_removed"
    - "GC scans all branches for reachability (not just current branch)"
  artifacts:
    - path: "src/tract/tract.py"
      provides: "compile(order=...) and gc() facade methods"
      contains: "def gc"
    - path: "src/tract/operations/compression.py"
      provides: "gc() operation and reorder safety checks"
      contains: "def gc"
    - path: "tests/test_reorder.py"
      provides: "Compile-time reordering tests"
      min_lines: 100
    - path: "tests/test_gc.py"
      provides: "Garbage collection tests"
      min_lines: 150
  key_links:
    - from: "src/tract/tract.py"
      to: "src/tract/operations/compression.py"
      via: "Tract.gc() delegates to gc() operation"
      pattern: "operations.compression.gc"
    - from: "src/tract/operations/compression.py"
      to: "src/tract/operations/dag.py"
      via: "GC uses dag.get_all_ancestors() for reachability analysis"
      pattern: "get_all_ancestors"
    - from: "src/tract/tract.py"
      to: "src/tract/protocols.py"
      via: "compile(order=...) reorders CompiledContext messages"
      pattern: "CompiledContext"
---

<objective>
Add compile-time commit reordering and garbage collection -- the final two capabilities of Phase 4.

Purpose: Reordering lets users control the order of compiled context without mutating history (COMP-03). GC lets users clean up unreachable commits with configurable retention policies (COMP-04). Together with compression (04-02), this completes all Phase 4 requirements.

Output: compile(order=...) with structural safety checks, gc() operation with retention policies, and comprehensive tests for both. All Phase 4 success criteria met.
</objective>

<execution_context>
@C:\Users\jinwi\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\jinwi\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-compression/04-CONTEXT.md
@.planning/phases/04-compression/04-RESEARCH.md
@.planning/phases/04-compression/04-01-SUMMARY.md
@.planning/phases/04-compression/04-02-SUMMARY.md

@src/tract/tract.py
@src/tract/operations/compression.py
@src/tract/operations/dag.py
@src/tract/protocols.py
@src/tract/models/compression.py
@src/tract/storage/repositories.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Compile-time reordering with safety checks</name>
  <files>
    src/tract/tract.py
    src/tract/operations/compression.py
    tests/test_reorder.py
  </files>
  <action>
    **operations/compression.py** -- Add reordering safety check functions:

    1. `check_reorder_safety(order, commit_repo, blob_repo) -> list[ReorderWarning]`:
       - Takes the ordered list of commit hashes and checks for structural issues
       - For each commit in order:
         a. If commit is an EDIT (operation == CommitOperation.EDIT) and its response_to target appears LATER in the order list: emit ReorderWarning(warning_type="edit_before_target", commit_hash=commit.commit_hash, description=f"EDIT commit {commit.commit_hash[:8]} appears before its target {commit.response_to[:8]}", severity="structural")
         b. If commit has response_to and the response_to commit is NOT in the order list at all: emit ReorderWarning(warning_type="response_chain_break", commit_hash=commit.commit_hash, description=f"Commit {commit.commit_hash[:8]} references {commit.response_to[:8]} which is not in the reordered set", severity="structural")
       - Returns list of ReorderWarning (may be empty)
       - Import ReorderWarning from tract.models.compression

    **tract.py** -- Extend `Tract.compile()`:

    1. Add `order: list[str] | None = None` parameter to `compile()` method signature
    2. Add `check_safety: bool = True` parameter
    3. After the existing compile logic produces a `result: CompiledContext`, add reordering block:

       ```python
       if order is not None:
           # Safety checks
           warnings = []
           if check_safety:
               from tract.operations.compression import check_reorder_safety
               warnings = check_reorder_safety(order, self._commit_repo, self._blob_repo)

           result = self._reorder_compiled(result, order)
           # Return tuple (CompiledContext, list[ReorderWarning]) when order is provided
           # NOTE: CompiledContext is @dataclass(frozen=True), so we CANNOT attach attributes.
           # Instead, return a tuple. Callers unpack: result, warnings = t.compile(order=...)
           return result, warnings
       ```

       IMPORTANT: When `order` is None (default), compile() returns `CompiledContext` as before (no tuple).
       When `order` is provided, compile() returns `tuple[CompiledContext, list[ReorderWarning]]`.
       This preserves backward compatibility while cleanly returning warnings.

    4. IMPORTANT: When `order` is provided, bypass the cache entirely. Move the order check BEFORE the cache lookup:
       ```python
       # If order provided, always do a full compile (bypass cache)
       if order is not None:
           result = self._compiler.compile(self._tract_id, current_head, ...)
           # Then reorder (as above)
           return result
       ```

    5. Add `_reorder_compiled(self, result, order)` private method:
       - Build `hash_to_idx = {h: i for i, h in enumerate(result.commit_hashes)}`
       - Validate all hashes in `order` exist in result.commit_hashes. Raise CommitNotFoundError for missing ones.
       - Compute `new_indices = [hash_to_idx[h] for h in order]`
       - Include any commits NOT in `order` at their original positions after the ordered ones:
         `remaining = [i for i in range(len(result.messages)) if i not in set(new_indices)]`
         `final_order = new_indices + remaining`
       - Reorder messages, generation_configs, and commit_hashes according to final_order
       - Recount tokens: `token_count = self._token_counter.count_messages([{"role": m.role, "content": m.content} for m in new_messages])`
       - Return new CompiledContext with reordered data and same commit_count, token_source

    **tests/test_reorder.py** -- Create test file:

    1. Basic reordering (~5 tests):
       - `test_compile_default_order`: compile() without order returns commits in chain order
       - `test_compile_reversed_order`: compile(order=[h3, h2, h1]) returns messages in reversed order
       - `test_compile_partial_order`: compile(order=[h3, h1]) puts h3 first, h1 second, h2 at end (not in order list). NOTE: Partial ordering is intentional -- commits not in `order` are appended at their original relative positions. Document this in the compile() docstring.
       - `test_compile_order_preserves_content`: reordered messages have same content, just different positions
       - `test_compile_order_recounts_tokens`: token count is recalculated for the new message order

    2. Safety checks (~5 tests):
       - `test_safety_edit_before_target`: Create commits where c1 is original, c2 is EDIT of c1. compile(order=[c2, c1]) produces warning with type="edit_before_target"
       - `test_safety_response_chain_break`: Create commit with response_to=c1, reorder without c1 -> warning
       - `test_safety_no_warnings_normal_order`: compile(order=[c1, c2, c3]) in original order -> no warnings
       - `test_safety_check_disabled`: compile(order=[c2, c1], check_safety=False) -> no warnings even with issues
       - `test_warnings_accessible`: compile(order=...) returns (result, warnings) tuple; warnings is a list of ReorderWarning objects

    3. Edge cases (~3 tests):
       - `test_compile_order_invalid_hash`: compile(order=["nonexistent"]) raises CommitNotFoundError
       - `test_compile_order_empty_list`: compile(order=[]) returns all messages in original order (empty order = no reordering effect, just remaining)
       - `test_compile_order_bypasses_cache`: compile() uses cache, compile(order=...) doesn't (verify different code paths via verify_cache flag or token count comparison after record_usage)
  </action>
  <verify>
    Run `python -m pytest tests/test_reorder.py -v` -- all ~13 new tests pass.
    Run `python -m pytest tests/ -x -q` -- all tests pass (no regressions from compile() signature change).
  </verify>
  <done>
    compile(order=...) reorders compiled messages. check_reorder_safety() detects structural issues (EDIT before target, response chain breaks). Cache bypassed for reordered compiles. ~13 tests covering reordering, safety checks, and edge cases.
  </done>
</task>

<task type="auto">
  <name>Task 2: Garbage collection operation + Tract facade</name>
  <files>
    src/tract/operations/compression.py
    src/tract/tract.py
    tests/test_gc.py
  </files>
  <action>
    **operations/compression.py** -- Add GC functions:

    1. `_get_all_reachable(tract_id, ref_repo, commit_repo, parent_repo, *, branch=None) -> set[str]`:
       - If `branch` specified: get that branch tip, walk ancestors
       - If no branch: get ALL branch tips via `ref_repo.list_branches()`, union all ancestors
       - Use `dag.get_all_ancestors(tip, commit_repo, parent_repo)` for each tip (DAG-aware BFS through both parent_hash and CommitParentRow -- NOT `commit_repo.get_ancestors()` which is first-parent only)
       - Also check HEAD if detached (may point to a commit not on any branch)
       - Returns set of reachable commit hashes

    2. `_get_all_tract_commits(commit_repo, tract_id) -> list[CommitRow]`:
       - Query all commits for the tract
       - This needs a new method on CommitRepository. Add `get_all(tract_id) -> Sequence[CommitRow]` to the ABC and SQLite impl.
       - Or: use the existing get_ancestors() from HEAD plus any orphaned commits. Simpler approach: query CommitRow directly where tract_id matches.

    3. `gc(tract_id, commit_repo, ref_repo, parent_repo, blob_repo, compression_repo, *, orphan_retention_days=7, archive_retention_days=None, branch=None) -> GCResult`:

       Flow:
       a. Record start time for duration_seconds
       b. Find all reachable commits via `_get_all_reachable()`
       c. Find all commits in tract (need to query all commits for this tract_id)
       d. Compute unreachable = all_commits - reachable
       e. Classify unreachable:
          - For each unreachable commit, check `compression_repo.is_source_of(commit_hash)`
          - If True: it's an archived commit (compression source)
          - If False: it's a truly orphaned commit
       f. Apply retention policies:
          - For orphans: remove if `(now - created_at).days >= orphan_retention_days`
          - For archives: remove only if `archive_retention_days is not None` AND `(now - created_at).days >= archive_retention_days`
       g. Delete eligible commits:
          - For each commit to remove:
            - Delete any CompressionSourceRow/CompressionResultRow referencing it
            - Delete the CommitRow
            - Check if the blob (content_hash) is still referenced by any other commit. If not, delete the BlobRow.
          - This requires adding `delete_commit(commit_hash)` to CommitRepository and `delete_if_orphaned(content_hash)` to BlobRepository (or handle in the gc function directly via session).
       h. Return GCResult(commits_removed, blobs_removed, tokens_freed, archives_removed, duration_seconds)

       IMPORTANT: Use `_normalize_dt()` pattern from existing codebase (strip tzinfo for SQLite naive datetime comparison) when comparing created_at against retention cutoff.

    **repositories.py + sqlite.py** -- Add needed methods:

    1. `CommitRepository.get_all(tract_id) -> Sequence[CommitRow]` (ABC + impl)
       - Returns all commits for a tract, ordered by created_at
    2. `CommitRepository.delete(commit_hash) -> None` (ABC + impl)
       - Deletes a commit by hash. Also clean up any CommitParentRow entries.
    3. `BlobRepository.delete_if_orphaned(content_hash) -> bool` (ABC + impl)
       - Checks if any CommitRow still references this content_hash
       - If not, deletes the BlobRow and returns True
       - If still referenced, returns False
    4. `CompressionRepository.delete_source(commit_hash) -> None` (ABC + impl)
       - Deletes CompressionSourceRow entries for a commit
    5. `CompressionRepository.delete_result(commit_hash) -> None` (ABC + impl)
       - Deletes CompressionResultRow entries for a commit

    **tract.py** -- Add `Tract.gc()`:

    ```python
    def gc(
        self,
        *,
        orphan_retention_days: int = 7,
        archive_retention_days: int | None = None,
        branch: str | None = None,
    ) -> GCResult:
    ```
    - Lazy import: `from tract.operations.compression import gc as _gc`
    - Call `_gc()` with all repos and params
    - Call `self._session.commit()`
    - Return GCResult

    **tests/test_gc.py** -- Create test file:

    1. Basic GC (~5 tests):
       - `test_gc_no_orphans`: Create linear commits, gc() returns commits_removed=0
       - `test_gc_removes_orphans`: Create commits, reset to earlier commit (making later ones orphaned), gc(orphan_retention_days=0) removes them
       - `test_gc_respects_retention`: Create orphans recently, gc(orphan_retention_days=7) does NOT remove them (too recent)
       - `test_gc_returns_stats`: Verify GCResult has correct counts (commits_removed, tokens_freed, blobs_removed)
       - `test_gc_removes_orphaned_blobs`: After removing orphan commits, their blobs are also removed if no other commit references them

    2. Compression archives (~4 tests):
       - `test_gc_preserves_archives_by_default`: After compress(), gc() does NOT remove compression source commits (archive_retention=None)
       - `test_gc_removes_old_archives`: gc(archive_retention_days=0) removes archived commits
       - `test_gc_archive_retention_threshold`: gc(archive_retention_days=30), recent archives preserved, old ones removed
       - `test_gc_archives_removed_count`: Verify GCResult.archives_removed reflects actual archive removals

    3. Multi-branch reachability (~3 tests):
       - `test_gc_respects_all_branches`: Commit reachable from feature branch but not main is NOT removed
       - `test_gc_branch_scoping`: gc(branch="main") only checks main's reachability, may remove commits reachable from other branches (but user explicitly scoped)
       - `test_gc_detached_head`: Commits reachable from detached HEAD are not removed

    4. Edge cases (~3 tests):
       - `test_gc_empty_tract`: gc() on empty tract returns all zeros
       - `test_gc_duration_positive`: GCResult.duration_seconds > 0
       - `test_gc_idempotent`: Running gc() twice produces same result (second run removes nothing)

    For orphan tests: Use `t.reset(commit_hash, mode="hard")` or branch manipulation to create unreachable commits. For archive tests: First compress(), then gc() -- the compressed source commits are archives.

    To make commits old enough for GC, either:
    - Set orphan_retention_days=0 in tests (immediate eligibility)
    - Or manually adjust created_at in the database (more realistic but heavier)
    Using orphan_retention_days=0 is simpler and sufficient for testing.
  </action>
  <verify>
    Run `python -m pytest tests/test_gc.py -v` -- all ~15 new tests pass.
    Run `python -m pytest tests/ -x -q` -- all tests pass (existing + reorder + gc, total ~563).
  </verify>
  <done>
    gc() operation with reachability analysis, retention policies, blob cleanup, and GCResult. Tract.gc() facade. CommitRepository.get_all() and delete(). BlobRepository.delete_if_orphaned(). ~15 GC tests covering basic removal, archive preservation, multi-branch reachability, and edge cases. Combined with Task 1, this completes COMP-03 and COMP-04.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_reorder.py tests/test_gc.py -v` -- all ~28 new tests pass
2. `python -m pytest tests/ -x -q` -- all tests pass (~563 total)
3. Verify COMP-03: compile(order=[h3, h1, h2]) produces messages in that order with safety warnings
4. Verify COMP-04: gc(orphan_retention_days=0) removes unreachable commits, returns GCResult with stats
5. Verify gc() preserves compression archives by default
6. Verify gc() respects all branches for reachability
</verification>

<success_criteria>
1. compile(order=...) reorders compiled messages without mutating DAG (COMP-03)
2. Structural safety checks detect EDIT-before-target and broken response chains
3. Reordered compiles bypass compile cache
4. gc() removes unreachable commits with configurable retention (COMP-04)
5. GC preserves compression archives by default (archive_retention=None)
6. GC scans all branches for reachability (not just current)
7. GCResult reports accurate stats
8. ~28 new tests; zero regressions on existing tests
9. All 4 Phase 4 success criteria from ROADMAP.md are satisfied across plans 04-01 through 04-03
</success_criteria>

<output>
After completion, create `.planning/phases/04-compression/04-03-SUMMARY.md`
</output>
